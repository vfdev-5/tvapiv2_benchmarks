Timestamp: 20221010-210127
Torch version: 1.14.0.dev20221010+cu116
Torchvision version: 0.15.0a0
Num threads: 1

[ Classification transforms measurements ]
                      |  stable  |    v2 
1 threads: ------------------------------
      PIL Image data  |  1.870   |  1.925

Times are in milliseconds (ms).

Traceback (most recent call last):
  File "main.py", line 1673, in <module>
    fire.Fire(
  File "/usr/local/lib/python3.8/dist-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/usr/local/lib/python3.8/dist-packages/fire/core.py", line 466, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/usr/local/lib/python3.8/dist-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "main.py", line 890, in main_classification_pil_vs_features
    bench_fn(opt, t_stable, t_v2, quiet=quiet, single_dtype="PIL", seed=seed, num_runs=20, num_loops=50)
  File "main.py", line 747, in bench_with_time
    transform(data)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/vision/torchvision/prototype/transforms/_container.py", line 18, in forward
    sample = transform(sample)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/vision/torchvision/prototype/transforms/_transform.py", line 38, in forward
    flat_outputs = [
  File "/vision/torchvision/prototype/transforms/_transform.py", line 39, in <listcomp>
    self._transform(inpt, params) if _isinstance(inpt, self._transformed_types) else inpt
  File "/vision/torchvision/prototype/transforms/_geometry.py", line 149, in _transform
    return F.resized_crop(
  File "/vision/torchvision/prototype/transforms/functional/_geometry.py", line 1323, in resized_crop
    return inpt.resized_crop(top, left, height, width, antialias=antialias, size=size, interpolation=interpolation)
  File "/vision/torchvision/prototype/features/_image.py", line 175, in resized_crop
    output = self._F.resized_crop_image_tensor(
  File "/vision/torchvision/prototype/transforms/functional/_geometry.py", line 1249, in resized_crop_image_tensor
    return resize_image_tensor(image, size, interpolation=interpolation, antialias=antialias)
  File "/vision/torchvision/prototype/transforms/functional/_geometry.py", line 128, in resize_image_tensor
    image = _FT.resize(
  File "/vision/torchvision/transforms/functional_tensor.py", line 469, in resize
    img = interpolate(img, size=size, mode=interpolation, align_corners=align_corners, antialias=antialias)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py", line 3945, in interpolate
    return torch._C._nn._upsample_bilinear2d_aa(input, output_size, align_corners, scale_factors)
KeyboardInterrupt
